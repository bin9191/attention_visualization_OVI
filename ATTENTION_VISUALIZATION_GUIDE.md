# Ovi Attention Visualization Guide

ë³¸ ë¬¸ì„œëŠ” Ovi ëª¨ë¸ì— Text-to-Video Cross Attention ì‹œê°í™” ê¸°ëŠ¥ì„ ì¶”ê°€í•œ ë‚´ìš©ì„ ì •ë¦¬í•œ ë¬¸ì„œì…ë‹ˆë‹¤.

## ğŸ“Œ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [ìƒˆë¡œ ì¶”ê°€ëœ íŒŒì¼](#ìƒˆë¡œ-ì¶”ê°€ëœ-íŒŒì¼)
3. [ìˆ˜ì •ëœ íŒŒì¼](#ìˆ˜ì •ëœ-íŒŒì¼)
4. [í•µì‹¬ ê¸°ìˆ ](#í•µì‹¬-ê¸°ìˆ )
5. [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
6. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)
7. [ë©”ëª¨ë¦¬ ìµœì í™”](#ë©”ëª¨ë¦¬-ìµœì í™”)

---

## ê°œìš”

Ovi ëª¨ë¸ì˜ text-to-video ìƒì„± ê³¼ì •ì—ì„œ íŠ¹ì • í…ìŠ¤íŠ¸ í† í°(ì˜ˆ: "man", "running")ì´ ë¹„ë””ì˜¤ í”„ë ˆì„ì˜ ì–´ëŠ ê³µê°„ì  ìœ„ì¹˜ì— attentioní•˜ëŠ”ì§€ë¥¼ ì‹œê°í™”í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥

- âœ… **Flash Attention â†” Standard Attention** config ê¸°ë°˜ í† ê¸€
- âœ… **íŠ¹ì • í† í° ì„ íƒ ì‹œê°í™”** (ì˜ˆ: "man", "running", "park")
- âœ… **Timestepë³„ attention ë³€í™”** GIFë¡œ ìƒì„± (50 timesteps)
- âœ… **ë©”ëª¨ë¦¬ ìµœì í™”**: 20GB RAMì—ì„œ ì‹¤í–‰ ê°€ëŠ¥ (íƒ€ì„ìŠ¤í…ë‹¹ 12.4MB)
- âœ… **ìë™ í† í° ì¸ë±ìŠ¤ ë¡œê¹…**: ì–´ë–¤ ì¸ë±ìŠ¤ê°€ ì–´ë–¤ ë‹¨ì–´ì¸ì§€ ìë™ í‘œì‹œ
- âœ… **íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ëª…**: ìƒì„± ì‹œê°ì´ í¬í•¨ëœ íŒŒì¼ëª…

### ìƒì„± ê²°ê³¼ë¬¼

```
A_man_running_at_the_park_512x992_103_0_20251102_193244.mp4
A_man_running_at_the_park_512x992_103_0_20251102_193244_attention_frame0_â–man.gif
```

- **MP4**: 121í”„ë ˆì„ì˜ ìƒì„±ëœ ë¹„ë””ì˜¤
- **GIF**: 50ê°œ diffusion timestepì— ê±¸ì¹œ attention map íˆíŠ¸ë§µ (ë¹¨ê°„ìƒ‰ = ë†’ì€ attention)

---

## ìƒˆë¡œ ì¶”ê°€ëœ íŒŒì¼

### `ovi/utils/ovi_attention_viz.py`

Attention ì €ì¥ ë° ì‹œê°í™”ë¥¼ ìœ„í•œ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆì…ë‹ˆë‹¤.

#### ì£¼ìš” í´ë˜ìŠ¤: `OviAttentionStore`

Text-to-Video cross attention weightsë¥¼ timestepë³„ë¡œ ì €ì¥í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.

**í•µì‹¬ ë©”ì„œë“œ**:

```python
class OviAttentionStore:
    def __init__(self, token_idx: Optional[int] = None):
        """
        Args:
            token_idx: ì‹œê°í™”í•  í…ìŠ¤íŠ¸ í† í° ì¸ë±ìŠ¤ (Noneì´ë©´ ì „ì²´ í‰ê· )
        """
        self.attention_maps = {}  # {timestep: {'spatial_map': [Lq], 'grid_sizes': ...}}
        self.token_idx = token_idx
        self.temp_spatial_sum = None  # Running averageë¥¼ ìœ„í•œ ì„ì‹œ ë²„í¼
        self.temp_layer_count = 0

    def store(self, attn_weights: torch.Tensor, q_shape, k_shape, grid_sizes):
        """
        Attention weightsë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            attn_weights: [B, H, Lq, Lk] í˜•íƒœì˜ attention weights
            q_shape: Query shape (video latent)
            k_shape: Key shape (text embeddings)
            grid_sizes: [B, 3] where 3=(F, H, W) - í”„ë ˆì„, ë†’ì´, ë„ˆë¹„ íŒ¨ì¹˜ ìˆ˜

        ë©”ëª¨ë¦¬ ìµœì í™”:
            1. GPUì—ì„œ ì¦‰ì‹œ spatial map [Lq] ì¶”ì¶œ
            2. CPUë¡œ ì¦‰ì‹œ ì „ì†¡
            3. Running average ëˆ„ì  (ë ˆì´ì–´ë³„ë¡œ í‰ê· í™”)
        """

    def set_timestep(self, t: int):
        """
        ìƒˆë¡œìš´ timestepìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.
        ì´ì „ timestepì˜ ëˆ„ì ëœ attentionì„ í‰ê· í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
        """

    def finalize(self):
        """ë§ˆì§€ë§‰ timestepì˜ attentionì„ ì €ì¥í•©ë‹ˆë‹¤."""
```

**ë©”ëª¨ë¦¬ ìµœì í™” ì „ëµ**:
- Full attention [B, H, Lq, Lk] ì €ì¥ ëŒ€ì‹  spatial map [Lq]ë§Œ ì¶”ì¶œ
- GPUâ†’CPU ì¦‰ì‹œ ì „ì†¡ìœ¼ë¡œ VRAM ì ˆì•½
- Layer-wise running averageë¡œ RAM ì ˆì•½

#### ì£¼ìš” í•¨ìˆ˜

**1. `visualize_attention_map()`**

```python
def visualize_attention_map(
    attn_data: dict,
    frame: Image.Image,
    frame_idx: int = 0
) -> Image.Image:
    """
    Spatial attention mapì„ 2D íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.

    Args:
        attn_data: {'spatial_map': [Lq], 'grid_sizes': [B, 3]}
        frame: ì›ë³¸ ë¹„ë””ì˜¤ í”„ë ˆì„ (PIL Image)
        frame_idx: ì‹œê°í™”í•  í”„ë ˆì„ ì¸ë±ìŠ¤

    Returns:
        Attention heatmapì´ ì˜¤ë²„ë ˆì´ëœ í”„ë ˆì„

    ì²˜ë¦¬ ê³¼ì •:
        1. grid_sizesì—ì„œ (F, H, W) ì¶”ì¶œ
        2. [Lq] â†’ [H*W] í•´ë‹¹ í”„ë ˆì„ì˜ attention ì¶”ì¶œ
        3. [H*W] â†’ [H, W] 2Dë¡œ reshape
        4. [H, W] â†’ [frame_height, frame_width] ë¦¬ì‚¬ì´ì¦ˆ
        5. Normalize & Colormap (ë¹¨ê°• = ë†’ìŒ, íŒŒë‘ = ë‚®ìŒ)
        6. ì›ë³¸ í”„ë ˆì„ì— ë¸”ë Œë”© (alpha=0.4)
    """
```

**2. `create_attention_video()`**

```python
def create_attention_video(
    attention_store: OviAttentionStore,
    frame: Image.Image,
    output_path: str,
    token_name: str = "",
    frame_idx: int = 0,
    fps: int = 10
):
    """
    ëª¨ë“  timestepì˜ attention mapì„ GIFë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        attention_store: ì €ì¥ëœ attention maps
        frame: ì›ë³¸ ë¹„ë””ì˜¤ í”„ë ˆì„
        output_path: GIF ì €ì¥ ê²½ë¡œ
        token_name: í† í° ì´ë¦„ (í‘œì‹œìš©)
        frame_idx: ì‹œê°í™”í•  í”„ë ˆì„ ì¸ë±ìŠ¤
        fps: GIF í”„ë ˆì„ë ˆì´íŠ¸

    ì¶œë ¥:
        50ê°œ í”„ë ˆì„ì˜ GIF (ê° diffusion timestepì˜ attention pattern)
    """
```

---

## ìˆ˜ì •ëœ íŒŒì¼

### 1. `ovi/configs/inference/inference_fusion.yaml`

Config íŒŒì¼ì— attention ì‹œê°í™” ì„¤ì •ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.

```yaml
# Flash Attention ì„¤ì •
use_flash_attention: false  # false = í‘œì¤€ attention ì‚¬ìš© (ì‹œê°í™” ê°€ëŠ¥)
                            # true = Flash Attention ì‚¬ìš© (ë¹ ë¥´ì§€ë§Œ ì‹œê°í™” ë¶ˆê°€)

# Attention Visualization ì„¤ì •
visualize_attention: true   # Attention ì‹œê°í™” í™œì„±í™”
visualize_token_idx: 1      # ì‹œê°í™”í•  í† í° ì¸ë±ìŠ¤
                            # 0 = "A", 1 = "man", 2 = "running", ...
                            # null = ì „ì²´ í† í°ì˜ í‰ê· 
visualize_frame_idx: 0      # ì‹œê°í™”í•  ë¹„ë””ì˜¤ í”„ë ˆì„ ì¸ë±ìŠ¤
                            # 0 = ì²« ë²ˆì§¸ í”„ë ˆì„
```

**ì£¼ì˜ì‚¬í•­**:
- `use_flash_attention: true`ì¼ ë•ŒëŠ” ì‹œê°í™”ê°€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤ (Flash Attentionì€ weightë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŒ)
- ì‹œê°í™”ë¥¼ ì›í•˜ë©´ ë°˜ë“œì‹œ `use_flash_attention: false`ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤

---

### 2. `ovi/modules/attention.py`

Flash Attentionê³¼ Standard Attentionì„ config ê¸°ë°˜ìœ¼ë¡œ ì „í™˜í•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.

#### ì¶”ê°€ëœ ì „ì—­ ë³€ìˆ˜ ë° í•¨ìˆ˜

```python
# ì „ì—­ Flash Attention í† ê¸€
USE_FLASH_ATTENTION = True

def set_flash_attention_enabled(enabled: bool):
    """
    Flash Attention ì‚¬ìš© ì—¬ë¶€ë¥¼ ì „ì—­ì ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

    Args:
        enabled: True = Flash Attention, False = Standard Attention
    """
    global USE_FLASH_ATTENTION
    USE_FLASH_ATTENTION = enabled
    if not enabled:
        warnings.warn(
            "Flash attention is disabled. Using PyTorch standard attention instead. "
            "This may result in slower performance and higher memory usage."
        )
```

#### ìˆ˜ì •ëœ `flash_attention()` í•¨ìˆ˜

```python
def flash_attention(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    k_lens: Optional[torch.Tensor] = None,
) -> torch.Tensor:
    """
    Configì— ë”°ë¼ Flash Attention ë˜ëŠ” Standard Attentionì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

    ì›ë˜ ì½”ë“œëŠ” Flash Attentionë§Œ ì‚¬ìš©í–ˆì§€ë§Œ,
    ì‹œê°í™”ê°€ í•„ìš”í•œ ê²½ìš° Standard Attentionìœ¼ë¡œ fallbackí•©ë‹ˆë‹¤.
    """
    # Configì—ì„œ Flash Attentionì´ ë¹„í™œì„±í™”ëœ ê²½ìš°
    if not USE_FLASH_ATTENTION:
        return attention(q, k, v, k_lens=k_lens)

    # Flash Attention 2 ë˜ëŠ” 3 ì‚¬ìš©
    # ... (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
```

#### ìˆ˜ì •ëœ `attention_with_weights()` í•¨ìˆ˜

```python
def attention_with_weights(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    k_lens: Optional[torch.Tensor] = None,
    average_for_q: bool = True,  # ìƒˆë¡œ ì¶”ê°€ëœ íŒŒë¼ë¯¸í„°
    total_video_latent_frames: int = 1,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Attention weightsë¥¼ í•¨ê»˜ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

    Args:
        average_for_q: Trueë©´ queryë³„ í‰ê·  attention ë°˜í™˜,
                      Falseë©´ ì „ì²´ attention matrix ë°˜í™˜

    Returns:
        out: Attention ì¶œë ¥ [B, Lq, C]
        avg_attn_weights: í‰ê·  attention weights
        attn_weights: ì „ì²´ attention weights [B, H, Lq, Lk]

    ì‹œê°í™”ë¥¼ ìœ„í•´ average_for_q=Falseë¡œ ì„¤ì •í•˜ë©´
    [B, H, Lq, Lk] í˜•íƒœì˜ ì „ì²´ attention matrixë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
```

---

### 3. `ovi/modules/fusion.py`

Fusion modelì˜ forward passì— attention ì €ì¥ ê¸°ëŠ¥ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.

#### ìˆ˜ì •ëœ í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜

ëª¨ë“  forward ê´€ë ¨ í•¨ìˆ˜ì— `store_attention` íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤:

```python
# 1. FusionModel.forward()
def forward(self, ..., store_attention=False):
    """
    Args:
        store_attention: Trueì´ë©´ attention weightsë¥¼ ì €ì¥
    """

# 2. single_fusion_block_forward()
def single_fusion_block_forward(self, ..., store_attention=False):
    """Block ë ˆë²¨ì—ì„œ store_attention í”Œë˜ê·¸ë¥¼ ì „ë‹¬"""

# 3. single_fusion_cross_attention_ffn_forward()
def single_fusion_cross_attention_ffn_forward(self, ..., store_attention=False):
    """Cross attention ë ˆì´ì–´ë¡œ í”Œë˜ê·¸ ì „ë‹¬"""

# 4. single_fusion_cross_attention_forward() - í•µì‹¬ ìˆ˜ì •
def single_fusion_cross_attention_forward(
    self,
    cross_attn_block,
    src_seq,
    src_grid_sizes,
    src_freqs,
    target_seq,
    target_seq_lens,
    target_grid_sizes,
    target_freqs,
    context,
    context_lens,
    store_attention=False  # ìƒˆë¡œ ì¶”ê°€
):
```

#### í•µì‹¬: Text-to-Video Cross Attention ì €ì¥ ë¡œì§

```python
def single_fusion_cross_attention_forward(self, ..., store_attention=False):
    b, n, d = src_seq.size(0), cross_attn_block.num_heads, cross_attn_block.head_dim

    # QKV projection
    if hasattr(cross_attn_block, "k_img"):
        q, k, v, k_img, v_img = cross_attn_block.qkv_fn(src_seq, context)
    else:
        q, k, v = cross_attn_block.qkv_fn(src_seq, context)
        k_img, v_img = None, None

    # ... (Sequence parallel ì²˜ë¦¬) ...

    # ========== Attention ê³„ì‚° ë° ì €ì¥ ==========
    if store_attention and \
       hasattr(cross_attn_block, 'attention_store') and \
       cross_attn_block.attention_store is not None:

        # ì‹œê°í™”ë¥¼ ìœ„í•´ attention_with_weights() ì‚¬ìš©
        from .attention import attention_with_weights

        x, _, full_attn_weights = attention_with_weights(
            q, k, v,
            k_lens=context_lens,
            average_for_q=False,  # ì „ì²´ [B, H, Lq, Lk] ë°˜í™˜
            total_video_latent_frames=31
        )

        # AttentionStoreì— ì €ì¥
        if hasattr(cross_attn_block.attention_store, 'store'):
            cross_attn_block.attention_store.store(
                full_attn_weights,  # [B, H, Lq, Lk]
                q.shape,            # Query shape
                k.shape,            # Key shape
                src_grid_sizes      # [B, 3] = [B, (F, H, W)]
            )
    else:
        # ê¸°ë³¸ ëª¨ë“œ: Flash Attention ì‚¬ìš© (ë¹ ë¦„, weight ì €ì¥ ì•ˆ í•¨)
        x = flash_attention(q, k, v, k_lens=context_lens)

    # ... (ë‚˜ë¨¸ì§€ cross attention ë¡œì§) ...
```

**ì£¼ìš” í¬ì¸íŠ¸**:
- `store_attention=True`ì¼ ë•Œë§Œ `attention_with_weights()` ì‚¬ìš©
- `store_attention=False`ì¼ ë•ŒëŠ” ê¸°ì¡´ì²˜ëŸ¼ `flash_attention()` ì‚¬ìš©
- `src_grid_sizes`ë¥¼ í•¨ê»˜ ì €ì¥í•˜ì—¬ spatial ì •ë³´ ë³´ì¡´

---

### 4. `ovi/ovi_fusion_engine.py`

Diffusion loopì—ì„œ timestepì„ AttentionStoreì— ì „ë‹¬í•˜ë„ë¡ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.

#### Diffusion Loop ìˆ˜ì •

```python
@torch.no_grad()
def generate(
    self,
    ...,
    attention_store: Optional[OviAttentionStore] = None,  # ìƒˆë¡œ ì¶”ê°€
):
    """
    Args:
        attention_store: Attention ì €ì¥ì„ ìœ„í•œ store ê°ì²´
    """

    # ... (ì´ˆê¸°í™”) ...

    # ========== Denoising Loop ==========
    for i, (t_v, t_a) in tqdm(enumerate(zip(timesteps_video, timesteps_audio))):

        # AttentionStoreì— í˜„ì¬ timestep ì„¤ì •
        if attention_store is not None:
            attention_store.set_timestep(int(t_v.item()))

        # ... (ë…¸ì´ì¦ˆ ì˜ˆì¸¡) ...

        # Forward pass with attention storage
        pos_forward_args = {
            'vid_e': latents_video,
            'vid_seq_lens': video_seq_lens,
            'vid_grid_sizes': video_grid_sizes,
            'vid_freqs': video_freqs,
            'audio_e': latents_audio,
            'audio_seq_lens': audio_seq_lens,
            'audio_grid_sizes': audio_grid_sizes,
            'audio_freqs': audio_freqs,
            'context': text_emb,
            'context_lens': text_lens,
            'store_attention': (attention_store is not None)  # í”Œë˜ê·¸ ì „ë‹¬
        }

        latents_video, latents_audio = self.model.forward(**pos_forward_args)

        # ... (ë…¸ì´ì¦ˆ ì—…ë°ì´íŠ¸) ...

    # ë§ˆì§€ë§‰ timestep ì €ì¥
    if attention_store is not None:
        attention_store.finalize()

    return latents_video, latents_audio
```

**ì£¼ìš” ë³€ê²½ì‚¬í•­**:
1. `attention_store` íŒŒë¼ë¯¸í„° ì¶”ê°€
2. ê° timestepë§ˆë‹¤ `set_timestep()` í˜¸ì¶œ
3. `store_attention` í”Œë˜ê·¸ë¥¼ forward passì— ì „ë‹¬
4. Loop ì¢…ë£Œ í›„ `finalize()` í˜¸ì¶œ

---

### 5. `inference.py`

ë©”ì¸ inference ìŠ¤í¬ë¦½íŠ¸ì— attention ì‹œê°í™” íŒŒì´í”„ë¼ì¸ì„ í†µí•©í–ˆìŠµë‹ˆë‹¤.

#### 1) Config ë¡œë“œ ë° Flash Attention ì„¤ì •

```python
# Configì—ì„œ ì„¤ì • ë¡œë“œ
visualize_attention = config.get("visualize_attention", False)
use_flash_attention = config.get("use_flash_attention", True)

# Flash Attention í† ê¸€
if not use_flash_attention:
    from ovi.modules.attention import set_flash_attention_enabled
    set_flash_attention_enabled(False)
    logger.info("Flash Attention: Disabled")
else:
    logger.info("Flash Attention: Enabled")
```

#### 2) AttentionStore ì´ˆê¸°í™”

```python
attention_store = None
if visualize_attention:
    from ovi.utils.ovi_attention_viz import OviAttentionStore

    token_idx = config.get("visualize_token_idx", None)
    attention_store = OviAttentionStore(token_idx=token_idx)
    attention_store.enable()

    # ëª¨ë“  video blockì— attention_store ë“±ë¡
    for block in ovi_engine.model.video_model.blocks:
        block.cross_attn.attention_store = attention_store

    logger.info(f"AttentionStore registered to {len(ovi_engine.model.video_model.blocks)} blocks")
```

#### 3) ë¹„ë””ì˜¤ ìƒì„± with Attention Storage

```python
# Generate video with attention tracking
generated_video, generated_audio = ovi_engine.generate(
    text_emb=text_emb,
    text_lens=text_lens,
    ...,
    attention_store=attention_store,  # AttentionStore ì „ë‹¬
)
```

#### 4) ë©”ëª¨ë¦¬ ìµœì í™”: í”„ë ˆì„ ë°±ì—…

```python
# ë¹„ë””ì˜¤ ì €ì¥ ì „ì— ì‹œê°í™”í•  í”„ë ˆì„ë§Œ ë°±ì—… (ë©”ëª¨ë¦¬ ì ˆì•½)
backup_frame = None
if visualize_attention and attention_store is not None:
    frame_idx = config.get("visualize_frame_idx", 0)
    if frame_idx < generated_video.shape[1]:
        backup_frame = generated_video[:, frame_idx, :, :].copy()
        logger.info(f"Backed up frame {frame_idx} for attention visualization")

# ë¹„ë””ì˜¤ ì €ì¥
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
output_path = f"{base_name}_{timestamp}.mp4"
save_video(output_path, generated_video, generated_audio, fps=24, sample_rate=16000)
logger.info(f"Video saved: {output_path}")

# ë©”ëª¨ë¦¬ í•´ì œ (ì¤‘ìš”!)
del generated_video
if generated_audio is not None:
    del generated_audio
import gc
gc.collect()
logger.info("Memory freed after video save")
```

**ë©”ëª¨ë¦¬ ìµœì í™” ì´ìœ **:
- `generated_video`: [3, 121, 512, 992] = ì•½ 180MB
- `save_video()`ì—ì„œ moviepyê°€ ëª¨ë“  í”„ë ˆì„ì„ listë¡œ ë³€í™˜ â†’ ë©”ëª¨ë¦¬ 2ë°°
- GIF ìƒì„±ì—ëŠ” ë‹¨ 1ê°œ í”„ë ˆì„ë§Œ í•„ìš” â†’ ë¯¸ë¦¬ ë°±ì—… í›„ ì‚­ì œ

#### 5) Attention GIF ìƒì„±

```python
if visualize_attention and attention_store is not None and backup_frame is not None:
    logger.info(f"Creating attention visualization with {len(attention_store.attention_maps)} timesteps...")

    # í† í° ì •ë³´ ë¡œê¹…
    tokens = ovi_engine.text_model.tokenizer.tokenizer.tokenize(text)
    logger.info("=" * 60)
    logger.info("Text Tokens and Indices:")
    for idx, token in enumerate(tokens):
        logger.info(f"  Index {idx:2d}: '{token}'")
    logger.info("=" * 60)

    # ë°±ì—…ëœ í”„ë ˆì„ì„ PIL Imageë¡œ ë³€í™˜
    frame = np.transpose(backup_frame, (1, 2, 0))  # [C, H, W] â†’ [H, W, C]
    frame = np.clip(frame * 255, 0, 255).astype(np.uint8)
    frame_pil = Image.fromarray(frame)

    # í† í° ì´ë¦„ ì¶”ì¶œ
    token_idx = config.get("visualize_token_idx", None)
    if token_idx is not None and token_idx < len(tokens):
        token_name = tokens[token_idx]
    else:
        token_name = "all_avg"

    # Attention GIF ìƒì„±
    from ovi.utils.ovi_attention_viz import create_attention_video

    frame_idx = config.get("visualize_frame_idx", 0)
    attn_output_path = f"{base_name}_{timestamp}_attention_frame{frame_idx}_{token_name}.gif"

    create_attention_video(
        attention_store=attention_store,
        frame=frame_pil,
        output_path=attn_output_path,
        token_name=token_name,
        frame_idx=frame_idx
    )

    logger.info(f"Attention GIF saved: {attn_output_path}")
```

**ì£¼ìš” í¬ì¸íŠ¸**:
1. í† í° ì¸ë±ìŠ¤ì™€ ì‹¤ì œ ë‹¨ì–´ë¥¼ ë¡œê·¸ë¡œ í‘œì‹œ (ì‚¬ìš©ì í¸ì˜)
2. ë°±ì—…ëœ í”„ë ˆì„ë§Œ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
3. íŒŒì¼ëª…ì— timestam + frame_idx + token_name í¬í•¨

---

## í•µì‹¬ ê¸°ìˆ 

### 1. Flash Attention vs Standard Attention

| íŠ¹ì§• | Flash Attention | Standard Attention |
|------|----------------|-------------------|
| ì†ë„ | ë¹ ë¦„ âš¡ | ëŠë¦¼ ğŸ¢ |
| ë©”ëª¨ë¦¬ (VRAM) | ì ìŒ ğŸ’š | ë§ìŒ ğŸ”´ |
| Attention Weight ì €ì¥ | âŒ ë¶ˆê°€ëŠ¥ | âœ… ê°€ëŠ¥ |
| ì‹œê°í™” | âŒ ë¶ˆê°€ëŠ¥ | âœ… ê°€ëŠ¥ |

**êµ¬í˜„ ë°©ì‹**:
```python
# Config ê¸°ë°˜ í† ê¸€
if USE_FLASH_ATTENTION:
    x = flash_attention_2_or_3(q, k, v)  # ë¹ ë¥´ì§€ë§Œ weight ì €ì¥ ì•ˆ ë¨
else:
    x = torch.nn.functional.scaled_dot_product_attention(q, k, v)  # weight ì¶”ì¶œ ê°€ëŠ¥
```

---

### 2. Cross Attention êµ¬ì¡°

Oviì˜ Text-to-Video Cross Attention:

```
Text Embeddings (K, V)     Video Latents (Q)
     [B, 512, C]     Ã—     [B, 61952, C]
                    â†“
            Attention Weights
              [B, H, Lq, Lk]
         [1, 24, 61952, 512]
                    â†“
        Spatial Map (íŠ¹ì • í† í°)
                 [Lq]
               [61952]
                    â†“
            Reshape to 2D
         [FÃ—HÃ—W] â†’ [HÃ—W]
       [31Ã—32Ã—62] â†’ [32Ã—62]
```

- **B**: Batch size (1)
- **H**: Attention heads (24)
- **Lq**: Query sequence length (ë¹„ë””ì˜¤ íŒ¨ì¹˜ ìˆ˜)
  - 31 frames Ã— 32 height patches Ã— 62 width patches = 61,952
- **Lk**: Key sequence length (í…ìŠ¤íŠ¸ í† í° ìˆ˜, ì˜ˆ: 512)

---

### 3. Spatial Map ì¶”ì¶œ

íŠ¹ì • í† í°ì˜ spatial attention patternì„ ì¶”ì¶œí•˜ëŠ” ê³¼ì •:

```python
# Input: attn_weights [B, H, Lq, Lk]
# ì˜ˆ: [1, 24, 61952, 512]

# 1. íŠ¹ì • í† í° ì„ íƒ (ì˜ˆ: token_idx=1 â†’ "man")
token_attn = attn_weights[0, :, :, token_idx]  # [H, Lq] = [24, 61952]

# 2. Multi-head í‰ê· 
spatial_map = token_attn.mean(dim=0)  # [Lq] = [61952]

# 3. GPU â†’ CPU ì¦‰ì‹œ ì „ì†¡ (VRAM ì ˆì•½)
spatial_map_cpu = spatial_map.detach().cpu()

# 4. Running average ëˆ„ì 
if temp_sum is None:
    temp_sum = spatial_map_cpu
else:
    temp_sum += spatial_map_cpu
layer_count += 1

# 5. Timestep ì™„ë£Œ ì‹œ í‰ê· í™”
avg_spatial_map = temp_sum / layer_count  # [61952]
```

---

### 4. 2D Spatial Map ì¬êµ¬ì„±

1ì°¨ì› spatial mapì„ 2D ì´ë¯¸ì§€ë¡œ ë³€í™˜:

```python
# Input: spatial_map [Lq] = [61952]
# grid_sizes [B, 3] = [[31, 32, 62]]  (F, H, W)

num_frames = 31
h_patches = 32
w_patches = 62
frame_idx = 0

# 1. í•´ë‹¹ í”„ë ˆì„ì˜ íŒ¨ì¹˜ë§Œ ì¶”ì¶œ
patches_per_frame = h_patches * w_patches  # 32 Ã— 62 = 1984
start_idx = frame_idx * patches_per_frame  # 0
end_idx = start_idx + patches_per_frame    # 1984

frame_attn = spatial_map[start_idx:end_idx]  # [1984]

# 2. 2Dë¡œ reshape
attn_map_2d = frame_attn.reshape(h_patches, w_patches)  # [32, 62]

# 3. ì›ë³¸ í”„ë ˆì„ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
attn_map_resized = cv2.resize(attn_map_2d, (992, 512))  # [512, 992]

# 4. Normalize (0-1)
attn_map_norm = (attn_map_resized - attn_map_resized.min()) / \
                (attn_map_resized.max() - attn_map_resized.min() + 1e-8)

# 5. Colormap ì ìš© (ë¹¨ê°• = ë†’ìŒ, íŒŒë‘ = ë‚®ìŒ)
heatmap = cv2.applyColorMap(
    (attn_map_norm * 255).astype(np.uint8),
    cv2.COLORMAP_JET
)

# 6. ì›ë³¸ í”„ë ˆì„ê³¼ ë¸”ë Œë”©
blended = cv2.addWeighted(frame, 0.6, heatmap, 0.4, 0)
```

---

## ì‚¬ìš© ë°©ë²•

### 1. Config ì„¤ì •

`ovi/configs/inference/inference_fusion.yaml` íŒŒì¼ ìˆ˜ì •:

```yaml
# Flash Attention ë¹„í™œì„±í™” (ì‹œê°í™”ë¥¼ ìœ„í•´ í•„ìˆ˜!)
use_flash_attention: false

# Attention ì‹œê°í™” í™œì„±í™”
visualize_attention: true

# ì‹œê°í™”í•  í† í° ì„ íƒ
visualize_token_idx: 1  # 0="A", 1="man", 2="running", 3="at", 4="the", 5="park"
                        # null = ì „ì²´ í‰ê· 

# ì‹œê°í™”í•  í”„ë ˆì„ ì„ íƒ
visualize_frame_idx: 0  # 0 = ì²« ë²ˆì§¸ í”„ë ˆì„

# í”„ë¡¬í”„íŠ¸ (í† í° ì¸ë±ìŠ¤ëŠ” ì´ í”„ë¡¬í”„íŠ¸ ê¸°ì¤€)
text: "A man running at the park"
```

### 2. ì‹¤í–‰

```bash
python3 inference.py --config-file ovi/configs/inference/inference_fusion.yaml
```

### 3. ì¶œë ¥ í™•ì¸

ìƒì„±ëœ íŒŒì¼:
```
outputs/
â”œâ”€â”€ A_man_running_at_the_park_512x992_103_0_20251102_193244.mp4
â””â”€â”€ A_man_running_at_the_park_512x992_103_0_20251102_193244_attention_frame0_â–man.gif
```

**GIF ë‚´ìš©**:
- 50ê°œ í”„ë ˆì„ (ê° diffusion timestepì˜ attention)
- "man" í† í°ì´ ì–´ë””ì— attendí•˜ëŠ”ì§€ íˆíŠ¸ë§µìœ¼ë¡œ í‘œì‹œ
- ë¹¨ê°„ìƒ‰ = ë†’ì€ attention, íŒŒë€ìƒ‰ = ë‚®ì€ attention

### 4. ë‹¤ë¥¸ í† í° ì‹œê°í™”

ì‹¤í–‰ ì‹œ ì½˜ì†”ì— í† í° ì¸ë±ìŠ¤ê°€ ìë™ìœ¼ë¡œ ë¡œê·¸ë©ë‹ˆë‹¤:

```
============================================================
Text Tokens and Indices:
  Index  0: 'â–A'
  Index  1: 'â–man'
  Index  2: 'â–running'
  Index  3: 'â–at'
  Index  4: 'â–the'
  Index  5: 'â–park'
============================================================
```

ì›í•˜ëŠ” í† í°ì˜ ì¸ë±ìŠ¤ë¥¼ í™•ì¸í•˜ê³  configë¥¼ ìˆ˜ì •:

```yaml
# "running" ì‹œê°í™”
visualize_token_idx: 2

# "park" ì‹œê°í™”
visualize_token_idx: 5

# ì „ì²´ í‰ê· 
visualize_token_idx: null
```

### 5. ë‹¤ë¥¸ í”„ë ˆì„ ì‹œê°í™”

```yaml
# ì¤‘ê°„ í”„ë ˆì„ ì‹œê°í™” (121í”„ë ˆì„ ì¤‘)
visualize_frame_idx: 60

# ë§ˆì§€ë§‰ í”„ë ˆì„ ì‹œê°í™”
visualize_frame_idx: 120
```

---

## ë¬¸ì œ í•´ê²°

ê°œë°œ ê³¼ì •ì—ì„œ ë°œìƒí•œ ì£¼ìš” ì—ëŸ¬ì™€ í•´ê²° ë°©ë²•ì…ë‹ˆë‹¤.

### 1. Flash Attention Import ê²½ê³ 

**ì—ëŸ¬**:
```
UserWarning: flash_attention imported but unused
```

**ì›ì¸**: `from .attention import flash_attention, attention` í›„ `attention()`ë§Œ ì‚¬ìš©

**í•´ê²°**: `flash_attention()` í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì¡°ê±´ë¶€ ë¶„ê¸°í•˜ë„ë¡ ìˆ˜ì •
```python
def flash_attention(...):
    if not USE_FLASH_ATTENTION:
        return attention(...)  # Fallback
    # Flash Attention êµ¬í˜„
```

---

### 2. Tokenizer AttributeError

**ì—ëŸ¬**:
```python
AttributeError: 'HuggingfaceTokenizer' object has no attribute 'tokenize'
```

**ì›ì¸**: Oviì˜ tokenizerëŠ” nested êµ¬ì¡°
```python
ovi_engine.text_model.tokenizer  # HuggingfaceTokenizer (wrapper)
ovi_engine.text_model.tokenizer.tokenizer  # AutoTokenizer (ì‹¤ì œ tokenizer)
```

**í•´ê²°**: Nested access ì‚¬ìš©
```python
# ì˜ëª»ëœ ë°©ë²•
tokens = ovi_engine.text_model.tokenizer.tokenize(text)

# ì˜¬ë°”ë¥¸ ë°©ë²•
tokens = ovi_engine.text_model.tokenizer.tokenizer.tokenize(text)
```

---

### 3. PIL Image ë³€í™˜ ì—ëŸ¬

**ì—ëŸ¬**:
```python
TypeError: Cannot handle this data type: (1, 1, 992), <f4
```

**ì›ì¸**: `generated_video`ë¥¼ ì§ì ‘ iterateí•˜ë©´ì„œ ì˜ëª»ëœ shape ì „ë‹¬

**í•´ê²°**: ì˜¬ë°”ë¥¸ indexingê³¼ transpose
```python
# ì˜ëª»ëœ ë°©ë²•
for frame in generated_video:  # frame shapeì´ ì´ìƒí•¨
    pil_frame = Image.fromarray(frame)

# ì˜¬ë°”ë¥¸ ë°©ë²•
frame = generated_video[:, frame_idx, :, :]  # [3, H, W]
frame = np.transpose(frame, (1, 2, 0))       # [H, W, 3]
frame = np.clip(frame * 255, 0, 255).astype(np.uint8)
pil_frame = Image.fromarray(frame)
```

---

### 4. RAM ë¶€ì¡± "Killed" ì—ëŸ¬

**ì—ëŸ¬**:
```
Killed  (process terminated by OS)
```

**ì›ì¸**: 20GB RAM ì œí•œì—ì„œ ë©”ëª¨ë¦¬ ë¶€ì¡±
- Full attention [B,H,Lq,Lk] ì €ì¥ ì‹œë„ â†’ 4.5TB í•„ìš”
- 121í”„ë ˆì„ì„ PILë¡œ ë³€í™˜ ì‹œ ë©”ëª¨ë¦¬ 2ë°°
- moviepy bufferë„ ë©”ëª¨ë¦¬ ì†Œë¹„

**í•´ê²°ì±… 1**: Immediate spatial map extraction
```python
# Before: ì „ì²´ ì €ì¥ (4.5TB)
self.attention_maps[timestep] = attn_weights  # [B, H, Lq, Lk]

# After: ì¦‰ì‹œ spatial map ì¶”ì¶œ (12.4MB)
spatial_map = attn_weights[0, :, :, token_idx].mean(dim=0)  # [Lq]
spatial_map_cpu = spatial_map.detach().cpu()
```

**í•´ê²°ì±… 2**: Running average
```python
# Before: ëª¨ë“  ë ˆì´ì–´ ì €ì¥ (30 layers Ã— 50 steps)
layer_attns = []
for layer in layers:
    layer_attns.append(attn_weights)

# After: Running average
if temp_sum is None:
    temp_sum = spatial_map
else:
    temp_sum += spatial_map
layer_count += 1
# Timestep ì™„ë£Œ ì‹œ í‰ê· í™”
avg = temp_sum / layer_count
```

**í•´ê²°ì±… 3**: í”„ë ˆì„ ë°±ì—…
```python
# Before: ì „ì²´ ë¹„ë””ì˜¤ ë©”ëª¨ë¦¬ì— ìœ ì§€
save_video(path, generated_video, ...)  # 180MB Ã— 2 (moviepy buffer)
create_gif(generated_video, ...)         # ì—¬ì „íˆ 360MB ì‚¬ìš©

# After: í•„ìš”í•œ í”„ë ˆì„ë§Œ ë°±ì—…
backup_frame = generated_video[:, 0, :, :].copy()  # 1.5MB
save_video(path, generated_video, ...)
del generated_video  # ë©”ëª¨ë¦¬ í•´ì œ
gc.collect()
create_gif(backup_frame, ...)  # 1.5MBë§Œ ì‚¬ìš©
```

---

### 5. FusionModel forward() íŒŒë¼ë¯¸í„° ì—ëŸ¬

**ì—ëŸ¬**:
```python
TypeError: FusionModel.forward() got an unexpected keyword argument 'store_attention'
```

**ì›ì¸**: Call chainì˜ ì¼ë¶€ í•¨ìˆ˜ì—ë§Œ `store_attention` íŒŒë¼ë¯¸í„° ì¶”ê°€

**í•´ê²°**: ëª¨ë“  í•¨ìˆ˜ì— íŒŒë¼ë¯¸í„° ì¶”ê°€
```python
# ìˆ˜ì •í•´ì•¼ í•  í•¨ìˆ˜ë“¤
FusionModel.forward(..., store_attention=False)
single_fusion_block_forward(..., store_attention=False)
single_fusion_cross_attention_ffn_forward(..., store_attention=False)
single_fusion_cross_attention_forward(..., store_attention=False)
```

---

### 6. grid_sizes Unpacking ì—ëŸ¬

**ì—ëŸ¬**:
```python
TypeError: cannot unpack non-iterable int object
```

**ì›ì¸**: `grid_sizes`ë¥¼ `[B, F, 2]` í˜•ì‹ìœ¼ë¡œ ì˜ëª» ê°€ì •
- ì‹¤ì œ: `[B, 3]` where 3 = (F, H, W)

**í•´ê²°**: ì˜¬ë°”ë¥¸ unpacking
```python
# ì˜ëª»ëœ ë°©ë²•
h_patches, w_patches = grid_sizes[0, frame_idx]  # frame_idxë¡œ indexing ë¶ˆê°€

# ì˜¬ë°”ë¥¸ ë°©ë²•
if len(grid_sizes.shape) == 2 and grid_sizes.shape[1] == 3:
    num_frames, h_patches, w_patches = grid_sizes[0].tolist()
    num_frames, h_patches, w_patches = int(num_frames), int(h_patches), int(w_patches)
```

---

### 7. BFloat16 NumPy ë³€í™˜ ì—ëŸ¬

**ì—ëŸ¬**:
```python
TypeError: Got unsupported ScalarType BFloat16
```

**ì›ì¸**: NumPyê°€ BFloat16ì„ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ

**í•´ê²°**: Float32ë¡œ ë³€í™˜ í›„ NumPy ë³€í™˜
```python
# ì˜ëª»ëœ ë°©ë²•
attn_map_2d = frame_attn.reshape(h_patches, w_patches).numpy()

# ì˜¬ë°”ë¥¸ ë°©ë²•
frame_attn = frame_attn.float()  # BFloat16 â†’ Float32
attn_map_2d = frame_attn.reshape(h_patches, w_patches).numpy()
```

---

## ë©”ëª¨ë¦¬ ìµœì í™”

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ

| ë°©ë²• | VRAM | RAM | ì´ ë©”ëª¨ë¦¬ | ì±„íƒ |
|------|------|-----|-----------|------|
| Full Attention ì €ì¥ | ~100GB | ~4.4TB | ~4.5TB | âŒ |
| Layer-averaged Attention | ~50GB | ~100GB | ~150GB | âŒ |
| **Running Avg + Spatial Map** | **~2GB** | **~620MB** | **~2.6GB** | âœ… |

### ìµœì¢… ì±„íƒ ì „ëµ

#### 1. Immediate Spatial Map Extraction

```python
def store(self, attn_weights, q_shape, k_shape, grid_sizes):
    """
    GPUì—ì„œ ì¦‰ì‹œ spatial map ì¶”ì¶œ
    [B, H, Lq, Lk] â†’ [Lq]
    """
    # 1. íŠ¹ì • í† í°ì˜ spatial map ì¶”ì¶œ (GPU)
    if self.token_idx is not None:
        spatial_map = attn_weights[0, :, :, self.token_idx].mean(dim=0)  # [Lq]
    else:
        spatial_map = attn_weights[0].mean(dim=(0, 2))  # [Lq]

    # 2. ì¦‰ì‹œ CPUë¡œ ì „ì†¡ (VRAM í•´ì œ)
    spatial_map_cpu = spatial_map.detach().cpu()

    # ë©”ëª¨ë¦¬ ì ˆì•½:
    # Before: [1, 24, 61952, 512] Ã— 4 bytes = 3.1GB (BFloat16ì´ë©´ 1.5GB)
    # After:  [61952] Ã— 4 bytes = 248KB
```

#### 2. Running Average Across Layers

```python
def store(self, spatial_map_cpu):
    """
    ë ˆì´ì–´ë³„ë¡œ ì¦‰ì‹œ ëˆ„ì í•˜ì—¬ í‰ê· í™”
    """
    # ëˆ„ì 
    if self.temp_spatial_sum is None:
        self.temp_spatial_sum = spatial_map_cpu
    else:
        self.temp_spatial_sum += spatial_map_cpu
    self.temp_layer_count += 1

    # ë©”ëª¨ë¦¬ ì ˆì•½:
    # Before: [61952] Ã— 30 layers = 7.4MB per timestep
    # After:  [61952] Ã— 1 (running sum) = 248KB per timestep
```

#### 3. Timestep Finalization

```python
def set_timestep(self, t: int):
    """
    Timestep ì „í™˜ ì‹œ ì´ì „ timestep í‰ê· í™”í•˜ì—¬ ì €ì¥
    """
    # ì´ì „ timestep í‰ê·  ê³„ì‚°
    if self.current_timestep is not None and self.temp_spatial_sum is not None:
        avg_spatial_map = self.temp_spatial_sum / max(self.temp_layer_count, 1)
        self.attention_maps[self.current_timestep] = {
            'spatial_map': avg_spatial_map,  # [61952]
            'grid_sizes': self.temp_grid_sizes
        }

    # ë¦¬ì…‹
    self.current_timestep = t
    self.temp_spatial_sum = None
    self.temp_layer_count = 0

    # ë©”ëª¨ë¦¬:
    # 50 timesteps Ã— 248KB = 12.4MB (ì „ì²´ ì €ì¥)
```

#### 4. Frame Backup Before Video Save

```python
# ë¹„ë””ì˜¤ ì €ì¥ ì „
backup_frame = generated_video[:, frame_idx, :, :].copy()  # 1.5MB

# ë¹„ë””ì˜¤ ì €ì¥ (ë©”ëª¨ë¦¬ 2ë°° ì‚¬ìš©)
save_video(output_path, generated_video, ...)  # 180MB â†’ 360MB

# ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ
del generated_video  # 180MB í•´ì œ
gc.collect()

# GIF ìƒì„± (ë°±ì—… í”„ë ˆì„ë§Œ ì‚¬ìš©)
create_attention_video(..., frame=backup_frame, ...)  # 1.5MB
```

### ìµœì¢… ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼

**VRAM (GPU)**:
- VAE: ~1.5GB
- Text Encoder (ì„ì‹œ): ~0.5GB
- Diffusion Model (ì„ì‹œ, CPU offload): ~0GB
- Spatial map extraction (ì„ì‹œ): ~3GB
- **Total**: ~2GB

**RAM (CPU)**:
- Models (CPU offload): ~11GB (5B params Ã— 2 bytes FP16)
- Generated video (ì„ì‹œ): ~180MB
- Attention maps (50 timesteps): ~12.4MB
- Backup frame: ~1.5MB
- **Total**: ~12GB
- **Peak during save_video()**: ~12.5GB

**20GB RAMì—ì„œ ì•ˆì „í•˜ê²Œ ì‹¤í–‰ ê°€ëŠ¥!** âœ…

---

## ì°¸ê³ ì‚¬í•­

### Attend-and-Excite ë…¼ë¬¸ê³¼ì˜ ë¹„êµ

**Attend-and-Excite**:
- Stable Diffusion (text-to-image)
- 16Ã—16 cross attention maps ì €ì¥
- ê° timestepë§ˆë‹¤ attention ê°•í™”/ì–µì œ

**Ovi Implementation**:
- Text-to-Video ìƒì„±
- 32Ã—62 spatial maps (512Ã—992 í•´ìƒë„)
- Attention ì‹œê°í™” ëª©ì  (ìˆ˜ì • ì—†ìŒ)
- 50 timesteps Ã— 31 framesì˜ ì‹œê³µê°„ ì •ë³´

### Flash Attention ë²„ì „

OviëŠ” Flash Attention 2ì™€ 3ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤:

```python
try:
    from flash_attn import flash_attn_func
    FLASH_VERSION = 2
except ImportError:
    try:
        from flash_attn_interface import flash_attn_func
        FLASH_VERSION = 3
    except ImportError:
        FLASH_VERSION = None
```

ì‹œê°í™” ì‹œì—ëŠ” ìë™ìœ¼ë¡œ PyTorch standard attentionìœ¼ë¡œ fallbackë©ë‹ˆë‹¤.

### íŒ¨ì¹˜ ê¸°ë°˜ êµ¬ì¡°

OviëŠ” ë¹„ë””ì˜¤ë¥¼ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤:

- **ì…ë ¥**: 512Ã—992 ë¹„ë””ì˜¤, 31í”„ë ˆì„
- **íŒ¨ì¹˜ í¬ê¸°**: 16Ã—16 pixels
- **íŒ¨ì¹˜ ìˆ˜**:
  - Height: 512 Ã· 16 = 32 patches
  - Width: 992 Ã· 16 = 62 patches
  - Frames: 31
  - **Total**: 31 Ã— 32 Ã— 62 = 61,952 patches

Attention mapì˜ ê° ê°’ì€ í•˜ë‚˜ì˜ íŒ¨ì¹˜(16Ã—16 ì˜ì—­)ì— ëŒ€í•œ attention ê°•ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

---

## ì¶”ê°€ ê°œì„  ì‚¬í•­ (í–¥í›„)

1. **Multi-token ì‹œê°í™”**: ì—¬ëŸ¬ í† í°ì„ ë™ì‹œì— ë‹¤ë¥¸ ìƒ‰ìœ¼ë¡œ í‘œì‹œ
2. **Temporal attention**: í”„ë ˆì„ ê°„ attention ì‹œê°í™”
3. **Interactive viewer**: ì›¹ ê¸°ë°˜ ì¸í„°ë™í‹°ë¸Œ ì‹œê°í™” ë„êµ¬
4. **Attention editing**: Attend-and-Exciteì²˜ëŸ¼ attention ì¡°ì‘ ê¸°ëŠ¥
5. **Comparison mode**: ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ì˜ attention ë¹„êµ

---

## ë¼ì´ì„ ìŠ¤

ë³¸ attention ì‹œê°í™” ê¸°ëŠ¥ì€ Ovi í”„ë¡œì íŠ¸ì˜ ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

---

## ë¬¸ì˜

ë¬¸ì œê°€ ë°œìƒí•˜ê±°ë‚˜ ê°œì„  ì‚¬í•­ì´ ìˆìœ¼ë©´ ì´ìŠˆë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”.

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-11-02
